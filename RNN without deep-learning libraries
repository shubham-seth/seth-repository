import numpy as np
import random

data = open('Shakespeare.txt', 'r').read()
chars = list(set(data))
data_size, vocab_size = len(data), len(chars)
print('data has %d characters, %d unique.' % (data_size, vocab_size))
char_to_ix = { ch:i for i,ch in enumerate(chars) }
ix_to_char = { i:ch for i,ch in enumerate(chars) }

hidden_layer_size=100
seq_length=25
learning_rate=30

def Loss(inputs, targets, hprev, Wxh, Whh, Why, bh, by):
    x={}
    y={}
    p={}
    h={}
    h[-1]=hprev
    loss=0
    for t in range(len(inputs)):
        x[t]=np.zeros((vocab_size,1))
        x[t][inputs[t]]=1
        h[t]=np.tanh(Wxh.dot(x[t])+Whh.dot(h[t-1])+bh)
        y[t]=Why.dot(h[t])+by
        p[t]=np.exp(y[t])/np.sum(np.exp(y[t]))
        loss+=-np.log(p[t][targets[t],0])
    dWxh=np.zeros_like(Wxh)
    dWhh=np.zeros_like(Whh)
    dWhy=np.zeros_like(Why)
    dbh=np.zeros_like(bh)
    dby=np.zeros_like(by)
    dhnext=np.zeros_like(h[0])
    for t in reversed(range(len(inputs))):
        dy=np.copy(p[t])
        dy[targets[t]]-=1
        dh=(Why.T).dot(dy)+dhnext
        dhraw=(1-h[t]*h[t])*dh
        dWhy+=dy.dot(h[t].T)
        dWxh+=dhraw.dot(x[t].T)
        dWhh+=dhraw.dot(h[t-1].T)
        dby+=dy
        dbh+=dhraw
        dhnext=(Whh.T).dot(dhraw)
    for dparams in [dWxh, dWhh, dWhy, dbh, dby]:\n",
        np.clip(dparams, -5, 5, out=dparams)
    return loss, dWxh, dWhh, dWhy, dbh, dby, h[len(inputs)-1]"
   
def sample(h, seed_ix, n, Wxh, Whh, Why, bh, by):
    x = np.zeros((vocab_size, 1))
    x[seed_ix] = 1
    ixes = []
    for t in range(n):
        h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)
        y = np.dot(Why, h) + by
        p = np.exp(y) / np.sum(np.exp(y))
        ix = np.random.choice(range(vocab_size), p=p.ravel())
        x = np.zeros((vocab_size, 1))
        x[ix] = 1
        ixes.append(ix)
    return ixes
   
Wxh=np.random.rand(hidden_layer_size, vocab_size)*0.01
Whh=np.random.rand(hidden_layer_size, hidden_layer_size)*0.01
Why=np.random.rand(vocab_size, hidden_layer_size)*0.01
bh=np.random.rand(hidden_layer_size, 1)*0.01
by=np.random.rand(vocab_size, 1)*0.01
       
mWxh=np.zeros_like(Wxh)
mWhh=np.zeros_like(Whh)
mWhy=np.zeros_like(Why)
mbh=np.zeros_like(bh)
mby=np.zeros_like(by)
hprev=np.zeros((hidden_layer_size,1))
smooth_loss=np.log(vocab_size)*seq_length
   
while n<200:
    inputs = [char_to_ix[ch] for ch in data[p:p+seq_length]]
    targets = [char_to_ix[ch] for ch in data[p+1:p+seq_length+1]]
    loss, dWxh, dWhh, dWhy, dbh, dby, hprev = Loss(inputs, targets, hprev, Wxh, Whh, Why, bh, by)
    smooth_loss = smooth_loss * 0.999 + loss * 0.001
    for params, dparams, mparams in zip([Wxh, Whh, Why, bh, by],[dWxh, dWhh, dWhy, dbh, dby],[mWxh, mWhh, mWhy, mbh, mby]):
        mparams+=(dparams*dparams)
        params+= -learning_rate*dparams/np.sqrt(mparams+1e-8)
    p+=seq_length
    if p+seq_length>len(data):
        n+=1
        p=0
        print(smooth_loss, n)
        sample_ix = sample(hprev, random.randint(1,vocab_size), 100, Wxh, Whh, Why, bh, by)
        txt = ''.join(ix_to_char[ix] for ix in sample_ix)
        print ('----\\n%s\\n----' % (txt,))
